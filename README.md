# idiotic-cross-entropy
At first, I thought this idea myself but I just found <br>
this is just a case of labeling trick using adaptive method (the others argue)<br>
I saw a paper relating this, but I can't find url <br>
<br>
The author of this paper(what I can't find now) argue that adaptive labeling trick helps to generalize metric(or loss), <br>
which means in the case that cross-entropy is used when training while l1-norm is used when testing, <br>
we can expect better result when using adaptive labeling trick <br>
<br>
I also saw a generalization effect of this trick (but which is not very strong)
